---
title: "資料的載入"
author: "Yao-Jen Kuo"
date: "July, 2016"
output:
  slidy_presentation:
    fig_width: 8
    fig_height: 5
---

## 讀取 csv

- 這是實務中最常運用的方法，csv 是 comma-separated values 的縮寫
- 學會使用絕對路徑或者相對路徑

```{r}
?read.csv#常用的重要函數
accidentList <- read.csv("/Users/tkuo/ntu_train/NTUTrainRL1/data/accident_list.csv", header=TRUE)
str(accidentList)
```

## 讀取 csv (2)

- 欄位屬性可以在讀取資料時設定
- 練習用相對路徑

```{r}
setwd("/Users/tkuo/ntu_train/NTUTrainRL1")#set working directory
accidentList <- read.csv("data/accident_list.csv", header=TRUE, sep=",", colClasses=c("character", "factor", "factor", "factor", "factor", "integer", "factor", "factor"))
str(accidentList)
```

## 載入 Excel 試算表

- 使用 `readxl` 套件
- `excel_sheets()` 函數可以顯示試算表清單

```{r, results='hide'}
#install.packages("readxl")
library(readxl)
excel_sheets("data/accident_list.xlsx")
```

## 載入 Excel 試算表 (2)

- `read_excel()` 函數可以載入試算表

```{r, results='hide'}
sheet <- read_excel("data/accident_list.xlsx")
?read_excel
```

## 載入 SAS 資料集

- 使用 `haven()` 套件
- `read_sas()` 函數可以載入 SAS 資料集

```{r}
#install.packages("haven")
library(haven)
sasdat <- read_sas("data/accident_list.sas7bdat")
```

## 抓取網頁資料

- 網頁爬蟲是一個專門的研究領域，這裡只有時間很快地簡介，有興趣的學員可以去選修專門為蟲友開的課程。一個完整的爬蟲流程通常會有以下程序:

1. Connector
2. Parser

## 工欲善其事

我們要先裝一些好用的 Chrome 外掛來幫助我們爬網頁。

- [Quick Javascript Switcher](https://chrome.google.com/webstore/detail/quick-javascript-switcher/geddoclleiomckbhadiaipdggiiccfje)
- [XPATH Helper](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=zh-TW)
- [JSONView](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc?hl=zh-TW)

## Connector

- 網頁資料藏在哪裡?
- 首先在 Chrome 瀏覽網頁中按 F12 叫出 Chrome 開發者介面, 點選 Network。通常你要爬的資料會藏在這 4 種資料中。
- Doc(Documents)
- XHR(XHR and Fetch)
- JS(Scripts)
- WS(Websockets)

## 來練習一下！

* [奇摩股市](https://tw.stock.yahoo.com/d/s/major_2330.html)
* Doc
* major_2330.html
  
* [批踢踢八卦版](https://www.ptt.cc/ask/over18?from=%2Fbbs%2FGossiping%2Findex.html)
* Doc
* index.html
  
* [PCHome購物中心](http://ecshweb.pchome.com.tw/search/v3.3/?q=sony&scope=all)
* XHR
* results?q=sony&page=1&sort=rnk/dc
* Request Method:GET(較簡單可以看到資料內容)
* http://ecshweb.pchome.com.tw/search/v3.3/all/results?q=sony&page=1&sort=rnk/dc
* 可以看到JSON格式的資料內容

## Parser

- 我們已經知道整包資料在哪裡，但多數時候我們不需要其他雜七雜八的資料，可以使用 XPath Selector 只將我們想要的資料選取出來。

- 按 Ctrl + Shift + X 叫出 XPath Helper
- 按住 Shift 將滑鼠游標移至網頁欲抓取的資料處

## 來練習一下!

- [奇摩股市](https://tw.stock.yahoo.com/d/s/major_2330.html)
- XPath Selector抓個股當日買超券商: //td[@class='ttt'][1]
    
- [批踢踢八卦版](https://www.ptt.cc/ask/over18?from=%2Fbbs%2FGossiping%2Findex.html)
- XPath Selector抓文章標題: //div[@class='title']/a
    
- [PCHome購物中心](http://ecshweb.pchome.com.tw/search/v3.3/?q=sony&scope=all)
- XPath Selector抓商品價格: //li/span/span

## 奇摩股市範例

```{r}
yahooStockRankParser <- function(n){
  library(magrittr)
  library(rvest)
  # 資料僅有股價排名前100的個股
  if (!n %in% 1:100){
    print("Parameter n should be a integer between 1 and 100")
  }else{
    URL <- "https://tw.stock.yahoo.com/d/i/rank.php?t=pri&e=tse&n=100" #網頁
    xpathRank <- "//table[2]/tbody/tr/td[1]"#排名的xpath
    xpathStock <- "//tbody/tr/td[@class='name']"#股票名稱的xpath
    xpathPrice <- "//table[2]/tbody/tr/td[3]"#股價的xpath
    doc <- read_html(URL, encoding="cp950")#將網頁讀進R
    # 用相同的方式擷取出需要的資訊
    rank <- doc %>% 
      html_nodes(.,xpath = xpathRank) %>%
      html_text
    stock <- doc %>% 
      html_nodes(.,xpath = xpathStock) %>%
      html_text %>%
      iconv(from = "UTF-8", to = "UTF-8")
    price <- doc %>% 
      html_nodes(.,xpath = xpathPrice) %>%
      html_text
    stockTmp <- data.frame(rank=as.integer(rank), stock=stock, price=as.numeric(price))
    stockDF <- head(stockTmp, n)
    return(stockDF)
  }
}
stockDF <- yahooStockRankParser(n = 101)#回傳訊息
stockDF <- yahooStockRankParser(n = 30)
```

## Reference
* R in Action, Robert I. Kabacoff
* The Art of R Programming, Norman Matloff
* 木刻思RCrawler

&copy; Tony Yao-Jen Kuo 2016